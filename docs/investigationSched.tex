\documentclass[]{scrartcl}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage[utf8]{inputenc}

%opening
\title{Orchestrating real-time containers:\\ on the possibilities of schedule manipulation}
\author{Florian Hofer}

\begin{document}

\maketitle

\begin{abstract}
In this document, an exploratory step by step investigation on process management and scheduling techniques are discussed.
%TODO: abstract? //  After some initial 
\end{abstract}

\section{Introduction}

%What is the state - Background, historical, terms and definitions
Emerging technologies such as the Internet of Things and Cloud Computing are re-shaping the structure and control of industrial processes radically. 
The extent of these innovations, which allow the creation of highly flexible production systems, is so wide that today we talk about a fourth industrial revolution.
Key enabling technologies such as distributed sensing, big-data analysis and cloud storage are taking the center stage in developing new industrial control systems.

The control of industrial processes, however, has not changed much over the last few decades, and there are reasons for it.
For instance, typical control applications found in industrial processes have to respond to changes in the physical world within predefined time limits.
Moving the execution of control tasks from devices physically co-located with the controlled process to cloud or fog computing platforms requires dealing with network delays that are difficult to predict.
Moreover, while bare-metal solutions lend the control design full authority over the environment in which its software will run, it is not straightforward to determine under what conditions the software can be executed on cloud computing platforms due to resource virtualization properties.
Yet, we believe that the principles of Industry 4.0 present a unique opportunity to explore complementing traditional automation components by a novel control architecture \cite{Tascietal2018}.

We believe that modern virtualization techniques such as application containerization \cite{Mogaetal2016,Tascietal2018,GoldschmidtHauck-Stattelmann2016} are key for adequate utilization of cloud computing resources in industrial control systems.
The use of containerized control applications would yield the same advantages that traditional containerized microservices present: light and easily distributable control applications would be able to, for instance, run on any system and at the same time, be easily maintained and updated. 
Thus, enhanced scalability, portability, updatability and availability can be achieved \cite{Fazioetal2016}.
%New -> Industry 4.0 relatedness..
Beyond the migration and resource savings, parallel operation of containers on devices such as PLCs and, in small amounts, on sensing and actuating field devices, is possible. 
This will increase reliability and robustness, while enabling further exploitation of Self-* properties including Self-awareness and Self-compare.
% same container on multiple devices = redundancy and reliability
% Ease maintenance, one system still up -> changes etc see Goldschmidtetal2018

% what do we do about it
In this paper, we explore the feasibility of managing real-time control applications running on a shared resource environment.
The contributions of this paper are: 
\begin{itemize}
	\item Evaluation of access and control techniques to vary a container's run parameters and constraints, including scheduling information;
	
	\item Exploration of implementation techniques of a task, which takes charge of automating those settings and constraints, further called orchestrator;
	
	\item Evaluation of monitoring and implementation of a monitoring interface;
	
	\item Static scheduling of application containers as real-time tasks;

	\item Latency and determinism tests to confirm parameters and configurations enabling static and (quasi) dynamic container management;
		
	\item Demonstration of how, under specific conditions, the same containers can be managed dynamically.
\end{itemize}

%Setup of the paper
The rest of this paper is structured as follows. Section~\ref{sec:relwork} analyzes related work, while Section~\ref{sec:context} introduces the problem and the environment.
We next investigate how an orchestration and resource management might be performed analyzing a variety of options in Section~\ref{sec:mgmttest}, and perform a test implementation of the chosen method in Section~\ref{sec:testimp}.
Finally, we document the performed performance tests in Section~\ref{sec:perftest} and conclude in Section~\ref{sec:conclusions}.

\section{Related work}
\label{sec:relwork}
%TODO: redo, view paper notes of the first edition

%Let's start with approaches containers for control applications
Containerizing control applications has been discussed in recent literature. 
Morga et al. \cite{Mogaetal2016}, for instance, presented the concept of containerization of full control applications as a means to decouple the hardware and software life-cycles of an industrial automation system.
Due to the performance overhead in hardware virtualization, the authors state that OS-level virtualization is a suitable technique to cope with automation system timing demands.
The authors propose two approaches to migrate a control application into containers on top of a patched real-time Linux-based operating system: 
\begin{itemize}
	\item A given system is decomposed into subsystems, where a set of sub-units performs a localized computation, which then is actuated through a global decision maker. 
	\item Devices are defined, where each component is an isolated standalone solution with a shared communication stack.
	Based on this, systems are further divided into modules, allowing a granular development and update strategy. 
\end{itemize}
The authors demonstrate the feasibility of real-time applications in conjunction with containerization, even though they express concern on the maturity of the technical solution presented.

In related work, Goldschmidt and Hauk-Stattelmann \cite{GoldschmidtHauck-Stattelmann2016} presented a similar solution.
They perform benchmark tests on modularized industrial Programmable Logic Controller (PLC) applications to analyze the impact of container-based virtualization on real-time constraints.
As there is no solution for legacy code migration of PLCs to this point, the migration to application containers could extend a system's lifetime beyond the physical device's limits.
Even though tests showed worst-case latencies of the order of 15ms on Intel-based hosts (this may prevent direct application), the authors argue that the container engines may be stripped down and optimized for real-time execution.
In a follow-up work of Goldschmidt et al. \cite{Goldschmidtetal2018}, a possible multi-purpose architecture has been detailed and the proposal tested in a real-world use case.
The results show worst case latencies in the range of 1ms for a Raspberry PI single board computer, making the solution viable for cycle times in the range of 100ms to 1s.
The authors state that topics like memory overhead, containers' restricted access and problems due to technology immaturity are still to be investigated.

Tasci et al. \cite{Tascietal2018} address architectural details not discussed in \cite{GoldschmidtHauck-Stattelmann2016} and \cite{Goldschmidtetal2018}, such as the definite run-time environment and how deterministic communication of containers and field devices may be achieved in a novel container-based architecture. 
They propose a Linux-based solution as host operating system, including both the single kernel preemption-focused PREEMPT-RT patch and the co-kernel oriented Xenomai. 
With the latter patch, this approach shows better predictability, although it suffers from security constraints. For this reason, they suggest limiting its application for safety-critical code execution. 
They analyze and discuss in detail inter-process messaging, focusing on the specific properties needed in real-time applications.
Finally, they implement an orchestration run-time managing intra-container communication and show that task times of $500\mu s$ are possible.

%What about containers in the cloud?
The three solutions discussed above share one common property: they were based on bare-metal configurations. 
The solutions took into consideration real-time constraints, but limited to execution on physical hardware.
Nonetheless, current trends in industry favor the flexibility of resource sharing through cloud computing.
Thus, there is reason to investigate whether real-time control applications may also be ran on a shared infrastructure, their capabilities and limitations.

In 2014, Garcia-Vallas et al. \cite{Garcia-Vallsetal2014} analyzed challenges for predictable and deterministic cloud computing.
Even though the focus of their paper is on soft real-time applications, certain aspects and limits can be applied to any real-time systems.
Merging cloud computing with real-time requirements is a challenging task; the authors state that the guest OS has only limited access to physical hardware and thus suffers from unpredictability of non-hierarchical scheduling, and thick stack communications. 

%TODO: missing work about scheduling of rt-tasks

%But there is noone investigating the combination
%TODO: add reference to ICAC paper if accepted
So far, we have seen that containerization techniques have been tested with early positive results in a variety of contexts.
However, a combination of containers executed on cloud resources and control application containerization has not yet been examined in the literature, left alone the management and orchestration of the latter. 
In this paper, we assess the feasibility of a centralized resource orchestrator for a static set of real-time enabled containers.


\section{Context and problem}
\label{sec:context}

With the rising needs of a more distributed system, the requirements for a running control tasks change as well.
Different from before, the new software architectures foresee a distributed deployment of single functionalities, requiring the nodes to work together all in sync for a production process to work properly. 
%As discussed previously, those systems incur in timing requirements, wic
%Hardrealtimecionstraints to insert here
The new setup also puts systems and architectures in front of new development requirements. Software modules are now to be exchanged and updates in a distributed manner as the functionality itself is not located in as single place anymore. 
Thus, requires for a change in controlling and management of these software modules, at the cost of additional complexity to increase maintainability and ease of use of such systems.

The trend to implement the single software modules into containers as isolated threads running in parallel on shared resource, reduces the hardware cost and increases software lifetime. 
This has the advantage of abstracting the controller implementation from its hardware host, making maintenance and redundancy quite easy to achieve. % rerfenrec to paper redundancy migration
However, these containers are not made to run with hard real-time constraints by default. Therefore, some changes at the operating system level as well as some additional orchestration have to be performed in order to reduce latencies and augment the level of obtained determinism.
The running containers may then be orchestrated statically, knowing all task, their actual execution time and periods, or dynamically where, as the tasks enter and leave, new schedules are computed on demand.
However, if the maximum peak execution time of one task is not known, the schedule of containers is undecidable. 

The containers are run using Docker this time and are started either by \texttt{runc} or docker's own \texttt{run}. 
If an \texttt{init} binary is specified, it will run as process number 1 allowing to capture and forward signals.
Each container has a different PID space and userspace than the parent system.
The mount points are separate and virtualized.
If we need to share the host's PID space, e.g. for the orchestrator, we can specify the flag \texttt{--pid=host}.
Until a real application image and data is available, the all-purpose software \texttt{rt-app} will be used to simulate the behavior of a set of real real-time containers. 
The configuration of RT-app is done via a JSON file. The file enables the specification of execution phases, including amount, type and duration of the computing simulation tasks.
%% In the program
For the purpose of out project, each container will have only one task. 
Thus, the configuration will contain only one entry for thread and a global configuration.
Times defined in the configuration are in $\mu s$.
%% Considerations
The configuration sample can be found in the git repository, inside the folder \texttt{prj/test\_cont}.

%POSIX Standard
The IEEE Std 1003.1 defines the interface every compliant operating system must have.
The standard also foresees, in addition to the main process management features, an optional X/Open System Interfaces XSI conformance.
The constants that define the presence and function of such interfaces might be found in the <unistd.h> header file. 
The values can be directly interrogated by verifying the constant as described in the standard.
XSI defines in particular three option groups: encryption, realtime (threads) and advanced realtime (threads). 
As verified, only a few of the advanced realtime features are available in a standard Linux System. 
Some conditional compiler flags may be used in the source code for a more detailed output of versions and capabilities. 
%Non standardized extensions
Interestingly, the POSIX standard does not define other scheduling types than \texttt{other}, \texttt{fifo} and \texttt{RR}. 
There is no mentioning of \texttt{deadline}(EDF), \texttt{iso}, or \texttt{batch}. 
At the same time, the standard foresees a structure defining the scheduler's parameters, but it contains only one parameter, the absolute scheduling priority.
This might need to be taken into consideration when trying to run the orchestrator in a non Linux environment.

%NOTE: add subsection additional for LINUX, ie pidof??

\section{Manipulating processes}
\label{sec:mgmttest}

In this part we are mainly interested in manipulating process priorities and scheduling behavior. 
The POSIX calls and standards are investigated to verify possibilities.

\subsection{PID namespaces}

Before considering option of sharing PID space with host, see if we can access from a privileged container nonetheless the namespace isolation. 
Posix defines functions ioctrl\_ns to determine the parent namespace of the actual one. can help getting the inode of a parent namespace and read information. 
If PIDs can also be read is a matter of investigation. Maybe it can be mount as \/procext?
What information can be read out from the pid is still to be investigated.

Manuals say that a parent can read child namespaces but not vice-versa. All inquiries of parent PID in fact once reached pid 1 (must not be 1 for a container) end up with a 0 response.

\subsection{PID information}
\label{sub:pidinf}

While in UNIX the PID table and information is part of the kernel memory, in linux the only way to access the process information is by parsing virtual files. 
If the orchestrator has to run on both systems, the PID retrieval must thus be implemented twice. Once via /proc pipe readout and once via sysctrl calls to read kernel memory.
It is highly likely that the same is true for the other resources, such as memory or connectivity. 
An example code has been tested which gives the PID's of all matching process names. 
It might therefore be possible to poll once and verify cyclically for unbound containers. 
A hook-like structure might be more convenient. Such modification would require a kernel change and is therefore considered as a backup solution for to follow in a second moment.

\subsection{Scheduling of processes}

As default there is only limited access to the scheduling activities of a process.
In the POSIX documentation we find the following functions:

\begin{itemize}
	\item \textbf{sched\_get\_priority\_max(int)}
		gets the max priority of the specified scheduling policy. On Linux, it is 0 for SCHED\_OTHER and 1 for everything else. 
	
	\item \textbf{sched\_get\_priority\_min(int)}
		gets the min priority of the specified scheduling policy. On Linux, it is 0 for SCHED\_OTHER and 99 for everything else. 
		
	\item \textbf{sched\_getparam(pid\_t, struct sched\_param *)}
		gets the scheduling parameters of a specified process ID
	
	\item \textbf{sched\_getscheduler(pid\_t)}
		gets the set scheduling policy for a PID
	
	\item \textbf{sched\_rr\_get\_interval(pid\_t, struct timespec *)}
		gets the length of the quantum used when using the round robin scheduling approach for realtime.
		With a Linux kernel, the round robin time slice is always 150 microseconds, and pid need not even be a real pid. At least that is what the manuals say. Interrestingly a test has shown that there are also 8000 or 16000 $\mu s$ sometimes returned when querying the value.
	
	\item \textbf{sched\_setparam(pid\_t, const struct sched\_param *)}
		sets the parameters for a specified process ID
	
	\item \textbf{sched\_setscheduler(pid\_t, int, const struct sched\_param *)}
		sets the scheduling policy for a PID
	
	\item \textbf{sched\_yield(void)}
		return the control to the scheduler (leave the execution state).
\end{itemize}

In addition, if the scheduling is set to traditional, i.e. SCHED\_OTHER, the niceness value for the complete fair scheduler can be adjusted with additional functions. 
POSIX as said, implements only three scheduling policies.
Linux in addition has now:

\begin{verbatim}
    #define SCHED\_BATCH		3
    /* SCHED\_ISO: reserved but not implemented yet */
    #define SCHED\_IDLE		5
    #define SCHED\_DEADLINE		6
\end{verbatim}


The newly introduced scheduling attributes are now extended as:

\begin{verbatim}
struct sched\_attr {
    __u32 size;
    
    __u32 sched\_policy;
    __u64 sched\_flags;
    
    /* SCHED\_NORMAL, SCHED\_BATCH */
    __s32 sched\_nice;
    
    /* SCHED\_FIFO, SCHED\_RR */
    __u32 sched_priority;
    
    /* SCHED\_DEADLINE */
    __u64 sched\_runtime;
    __u64 sched\_deadline;
    __u64 sched\_period;
};
\end{verbatim}

And might be read and written through two Linux specific system calls which are specified as follows:


\begin{verbatim}
int sched_setattr(pid_t pid, struct sched_attr *attr,
    unsigned int flags);

int sched_getattr(pid_t pid, struct sched_attr *attr,
    unsigned int size, unsigned int flags);
\end{verbatim}

This new data structure is an addition to the original sched\_param which has been kept for standardization and backwards compatibility issues.

The commented scheduling mode in the list, i.e. SCHED\_ISO, is an algorithm intended for isochronous (burst-like) tasks and has not been completely implemented yet. 
The brainfuck scheduler (BFS) developed by Con Kolivas is alternative version witch actually supports the ISO scheduling mode. The new processes will have a priority between RT and Normal/Other tasks.
For further details see "A complete guide to Linux process scheduling"  by Nikita Ishkov.

\subsection{Other considerations}

During the exploration of the namespace and PID function, some features to be implemented in future where discovered. 
The PID 1 inside the container is parent of all the namespace. If it dies, all the children are orphaned and will be killed. As parent, it is also target of sigs without implicit binding, thus it must manually handle incoming signals such as sig\_int.
The userspace in the container can be mapped. To avoid root as well as other accesses from a container, a proper map should be set up and incorporated in all the containers at load.

\section{Test implementation}
\label{sec:testimp}

The orchestrator will have to interact with the scheduler while monitoring the tasks. 
Ideally, the interaction is mutual. Unfortunately, no hint has been found suggesting that there is a way to ``register'' to the scheduler to get scheduling and process updates. 
Thus, apart from the system calls for getting and setting a process's scheduling attributes, there is no direct communication with the kernel. 
This means that the PIDs of all the processes must be queried manually, cyclically, considering time trends and thus the orchestrator must keep PID history. 

\subsection{PID search}

%% how
In a first implementation test, we verify therefore the poll quality and speed of pid queries.
As described in Section \ref{sub:pidinf}, there are two ways of polling the process table. 
In our testing environment we use Linux, and are therefore running for a virtual file-systems parsing. 
To ease the parsing, we can use the integrated tool ``pidof'' via a pipe opened through the call of \texttt{popen}.
\texttt{popen} is a POSIX standard call that opens a one-way pipe to indicated purposely process called via shell.
The processes can either be identified directly or via its parent process. Every container is in fact starting a daemon PID called 'docker-containerd-shim'. This eases grouping and process identification and is preferred to the previous solution.
Another option is to scan the configured control groups for PIDs. Every container in Docker gets assigned its own control group. Therefore, each of these entries contains the list of PIDs running inside a running container. 
The latter technique is simpler and allows direct identification of containers.
All three modes will be implemented in the final version, leaving the choice of operation either to the user or to automatic detection.

%% in the programm
As soon as the orchestrator is up and running, it scans for target processes.
The PID list is parsed and verified for matching process signatures or containers.
Valid PIDs will be stored in memory for further processing.
The whole process takes a fraction of a ms and does not put load on the {CPU}. With unlimited loop the {CPU}-load is below 3\% (example taken from pidof).

%% Consequences and results>>
An option to optimize process verification might be that the environment controlling the containers informs the orchestrator about containers entering or leaving the scheduler.
This way, an orchestrator does not have to scan for new PID all the time.
Kubernetes, for example, foresees the use of postStart and preStop handlers when installing a pod. 
The handlers can be specified in the yaml configuration file of the POD for each container. A Typical use for this might be the shutting down or preparation for a service such as a web server. 
In our case we could use it to signal a process start or stop.
More details about this configuration can be \href{{https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/}}{found here}.
The Docker API on the other hand does not implement a pre- or post execution handler for container start or stop. 
Only the container configuration itself could contain a run command as part of the container to be executed upon startup. This might be also a standard configuration setting, signaling the orchestrator the presence of a new container process.
Anyway, this will be a feature to be implemented in a second moment.

The test development is stored in git under {src/testPosix/thread.c}.

\subsection{PID list}

%% how
Once the PIDs have been acquired, the orchestrator has to maintain a list of active processes. 
For this purpose, and in order to match PIDs with scheduling information, a dynamic memory structure may be used.
In order to store the information, the sorted resulting PID number array is sequentially used to fill a dynamic structure with scheduling data. 
Cyclically, the memory structure will be compared with the updated PIDs. New processes will be inserted in order in the structure, removed PIDs will be dropped.
The actual type of dynamic data structure is irrelevant. Possible structures are linked lists, double linked lists, as well as sorted trees of any kind.

%% In the program
To test an implementation, a simple linked list data structure has been implemented as a separate source file {src/testPosix/pidlist.c}.
The test program structure has been rewritten. Now two threads are launched to keep track of the running processes. 
Thread 1 will keep track of PIDs, add and remove list elements. The resulting PIDs array of a periodic scan is compared to the list's content.
Thread 2, implemented in a second step, will take care of the distribution of resources among the listed PIDs. 

%% Considerations.
The test implementation of Thread 1 works fluently. For the final version of the orchestrator, Kernel list based structures should be considered. 
The standard Linux linked lists use predefined macros for add and delete in the list. Differently from the managed list, this implementation does not cover the memory management of the structure. Memory allocation and freeing must still be done by the programmer.

\subsection{PID scheduling}
 
%% how
As a final step for the basic PID scheduling, we gather the information of a target process and set the new attributes. 
Resources, initially only {CPU}-affinity, are distributed according to configured settings, i.e. max values for resource occupation. 
This is for example a 80\% boundary for each computation unit ({CPU}). 
The processes and its running parameters might be defined in a configuration file (\textit{cmdfile}). If not, the real-time parameters of the process are read and stored.
% -> connected
The daemon and other system tasks should be mapped to separate resources. Even though real-time tasks have higher priority, a separate CPU might be needed to keep enough boundary for new entries and maintain the system responsive.
Affinity selection can be done via the functions \texttt{sched\_setaffinity} and \texttt{sched\_getaffinity}.
In addition, a variety of auxiliary functions are also available to manage those. 
The macro commands starting with \texttt{CPU\_*} may be used to create and modify CPU-Set masks, needed to change a process's affinity.

%% In the programm
A CPU has a standard cyclic computation monitoring unit of 1 ms, separating it into 1000000 ticks. 
The different run-times, deadlines and periods of the periodic tasks will be used to find a common denominator on a 1ms basis to create a proper configuration file. 
The program will then inspect entering and leaving processes, and bind them to the specified resources.
%In the end, e.g. 80\% of {CPU} are filled cyclically. 
%The different distribution strategies, i.e. round robin etc., might be implemented in the final product.
In the program the new entries found by thread two will be checked for affinity and resources.
Once the task information has been gathered, the updated values will be set according to the actual configuration.
%The distribution and cap of resources might be defined as round robin or alike.
During this phase, thread two will also read actual values and monitor the execution cap of tasks.

%% Considerations
Thread 2 with be responsible for this task. Cyclically, and in a second moment by trigger, the new PIDs are configured to associate with specific resources.
As this might introduce some resource peak, the reallocation must happen as soon as possible. 

The new process entry might use any of the resouces and thus add some unwanted peak load. 
If possible, the daemon and consequently the new created container processes should be bound to a reserved {CPU}, where all the system tasks are also allocated.

\subsection{Parameters and configuration}

%% How
Parameter configuration can be done in multiple ways. The two major options taken into consideration are parameter file and command line parameters.
The latter might work for a test phase, but might get to be a burden once the system and configurable parameters grow in size.
%A combination of both might be the ideal solution for the final product, allowing resource configuration via file, but at the same time, execution parametrization.

%% In the program
For some first tests, command line parameters will be used to set PIDs, their affinity, global thresholds and limits.
Those will later be moved into a configuration file.
The parameters include Ulimit, CPU masks of reserved computing units, memory maps and I/O resources that might be configured. 
For the implementation, the posix \texttt{getopt} parsing function will be used.
To parse the configuration file, a predefined library will be used where possible.

%% Considerations
When working with strings and buffers, such as configuration files, particular attention must be given to the possibility of buffer overflows and alike.
This is a general weakness as documented in the CWE database. % add here reference to CWE
Finally, the combination of configuration file containing all process details and the command arguments, needed for example to select configuration file and set verbosity, might be the best option.

\subsection{Logging and internal monitoring}

%%% How
RT-app includes a logging function to keep track of all tasks activity during execution. The logging itself writes to a file without rotation or buffers.
This thus does not allow to use the default logging function for continuous monitoring purposes.
If a ring-buffer is set in the configuration settings, the buffer is written to file only once the test is finished or interrupted. 
Thus, an adaptation of the logging function might be needed to allow readout of the task's performance.
The source code is available and written in C. Changes should therefore be no major problem.

%%% In the program
Some minor changes to the logging function have been successfully tested. Insertion of the ring-buffer dump every time it's capacity has been reached and overwriting of the log-file works as expected.
Issues that might be encountered are the high load to disk I/O if a small disks buffer size forces to write to file, and consequently induces delays.
%
%%% Considerations
An option to consider might be the writing to the stdout or a named FIFO buffer and piping the process to the Icinga plugin. 

\subsection{Icinga plugin}

%% How
To monitor the app, the interaction of orchestrator and the monitoring software Icinga is foreseen.
Icinga is a host-client system and relies on daemons and plugins on-site to exchange data. 
To reduce the overhead for the system, the orchestrator will share the obtained process details from the scheduler with the plugin running on our real-time host.
In addition, Docker creates CGroups for the running containers, giving the monitoring process access to useful network and memory information.
Also, possible might be to read disk I/O of a container, even though it could be that it is not necessary. 
The Icinga plugin made for Docker available online has a few limitations. In particular, it is limited to standard values such as CPU usage and network throughput.
Additional statistics, such as deadline miss or monitoring of process execution itself might be acquired via a modified version of the plugin. 

%
%% In the program

%
%% Considerations
The instructions for a \href{{https://nagios-plugins.org/doc/guidelines.html#DEVREQUIREMENTS}}{Nagions plugin}, the open source parent of Icinga, are used to create the proper plugin output.
The test development is stored in git under {src/check\_dockerRT/}.

\subsection{Thread and process communication}

%% How
Finally, we have to specify how threads and tasks may communicate.
The communication between the running software can be performed in different ways:

\begin{enumerate}
	\item we might use the usual IPC, as it is done between processes via
	\begin{enumerate}
		\item pipe - a connection between in and out, therefore not easily doable as the threads by default share the same stdin and stdout, can be used only for data streams;
		\item fifo (or named pipe) - similar to pipes, but rely on the file system to be able to connect two arbitrary processes in the system, may be used to talk bidirectionally; % http://www.embhack.com/pipe-and-fifo/
		\item signals - such as \textit{USR1} or \textit{USR2} which might be sent between processes, but hardly doable on the same process but different threads. The process would need to signal itself and the amount of information exchanged is contained int the signal itself;
		\item sockets - might be used as a medium where both threads access externally to this resource and exchange information, can be used for streams and datagramms;
	\end{enumerate}
	\item shared memory - probably the easiest and fastest type. The advantage in case of threads is that they share memory. Thus the only additional feature we need are locks/semaphores. Then, data can be exchanged in shared locations or FIFO buffers.
\end{enumerate}
The easiest approach for the orchestrator's threads is to use shared memory and a semaphore/mutex. 
On the other hand, sockets and fifo pipes are more flexible. Thus, the latter will be used for the communication of orchestrator and monitoring plugin.

%% In the program
In our test implementation we add a pthread\_mutex\_t to be acquired before every access to the PID list.
This way it is possible to instantiate a simple mechanism of intrinsic communication, without having to specify any additional data and protocol.
The program changes are minimal and thus the code remains as readable as before.
A fifo buffer, or a named pipe, will be used to pass run statistics to the planned Icinga plugin.
The pipe is simply opened creating a virtual file descriptor and connect both communication partners.  Management and information passing is done as if it were a file.

%% Considerations
The two threads can manage the list independently as before. On every execution the actual thread has exclusive access to the list.
This way, thread one can drop any removed item simply from the list and add new PIDs once discovered. 
Thread two has to scan the PID list anyway.
If new items are found, the settings are populated and resources allocated and scheduled.

\section{Implementation performance tests}
\label{sec:perftest}

%TODO:
In this section we detail function, implementation and tests of the orchestrator.
First we discuss implemented functionality and assumptions taken. Second, we describe the structure of the program, detailed by source file. 
Third, a test setup and results are described.

\subsection{Function principle}

The orchestrator's primary goal is to allocate container resources to maintain determinism at best.
To achieve this, containers and their running processes have to be confined in their execution context. 
We have seen in Section \ref{sec:mgmttest} a variety valid techniques to obtain such isolation. 
Section \ref{sec:testimp} used these techniques to explore the applications of those to check, maintain and monitor running containers.
In the following we will describe the final implementation and its functions.

%% Considerations
% Considerations wrong for the static version of the scheduler
%This is the last active section of the test implementation and probably the most complicated as well.
%The ideal resource management is hard to guarantee.
%The thread has to calculate utilization values and utilization limits, things known from real time scheduling theory, multiple times a second. 
%If we do not consider interdependence of the threads, the scheduling algorithm might be simplified to operate on multiple single core units with dedicated memory and I/O.
%If the tasks are instead depending on each other, more complex and sophisticated algorithms are needed.
%Such algorithms must surely be reevaluated for the dynamic scheduling version.

static algorightm


Static with dynamic entry

-differences to static 
-why, application 
-advantanges.. 

->maybe todo introduce kubernetes in the equation 
->architecture-, maybe also in the intro


dynamic algorithm

%TESTS STATIC AND DYNAMIC

The dynamic scheduling will therefore be done at best using the following considerations.
For simplicity, we suppose that the real-time software running in the various containers is all independent. 
The processes will be considered as black boxes where we do neither have specifications, nor  source code. 
There exist quasi static approaches to improve the schedulability, but they suffer from an openness constraint.
This also means that quasi-static scheduling and similar approaches that opt to improve schedulability at compile-time are not applicable.
The tasks may exceed the deadline, but in low probability and contemporaneity
we have to consider defining those values in order to obtain boundaries and limits. Might also be experimental values obtained from the running containers in Siemens Business Unit Farm

%GENERAL NOTES 
%Heuristic bound, wc has to be satisfied
%do on the go within boundaries, otherwise is undecideable
%need to get the bounds
%Scheduling algorithm with known bounds
%rendezvous mechanism to go 
%way of achieving hard 
%-> constraints into a cost function
%
%
%Sophisticate -> eplsylon out of bound is added cost -> calculate alg
%
%Container
%Crap -> container - run test
%
%C -> binary code can be linked to the final binary to increase the efficiency of the whole thing
%
%Reduce the load of the operating system -> quasi static system, decision, compile policy together with the thing.
%
%paper alberto -> scheduling integer linear,
%-> forward 
%ballarin, 


% Time syncronous network (TSN) maybe necessary

\subsection{Test configuration}


# insert specifications of eah 

\subsubsection{rt-app}

parameters: run, runtime -> consequent test overrun.

#
scripts and containers
scripts for tests and test-runs

\subsection{Static scheduler - structure}

divided in 3 main files, 
#main 
the progam start , defines enviroment parameters. -> runtime etc. interval , opt run
setup and configuration of system, cores etc
#update 
scans for containers and keeps statistics up to date
#manage
assigns resources to containers. Algothm of allocation and container configuration resides here

\subsection{Static scheduler - performance tests}

Tests run on the bare metal machine
test T3
test C5
Test T3U?

Different configurations
-> maybe start from 30% -. 10 step ?
upto 100%

test-run with multiple rt-cores in parallel .. ->> same tests again

Results = differences and conclusions

\subsection{Adaptive scheduler -  structure}

Changes to the program. From before to now


\subsection{Dynamic Scheduler - structure}


\section{Conclusions}
\label{sec:conclusions}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibliography,paper3}

\end{document}
