\documentclass[]{scrartcl}
\usepackage{hyperref}

%opening
\title{Investigation on possibilities of scheduling manipulation}
\author{Florian Hofer}

\begin{document}

\maketitle

%\begin{abstract}
%
%\end{abstract}

\section{Environment}

Containers are run, started either by runc or dockerrun, and initialized if configured with an init thread as process 1. 
The new container has a different pid space and userspace
separate mount points for volumes. docker we can specify --pid=host to set to a shared userspace.

\subsection{POSIX Standard}

The IEEE Std 1003.1 defines the interface every compliant operating system must have. The standard also foresees, in addition to the main process management features, an optional X/Open System Interfaces XSI conformance.
The constants that define the presence and function of such interfaces might be found in the <unistd.h> header file. 
The values can be directly interrogated by verifying the constant as described in the standard.
XSI defines in particular three option groups: encryption, realtime (threads) and advanced realtime (threads). 

as verified, only a few of the advanced realtime features are available. 
The conditional compiler flags might be used in the sourcecode for a more detailed outputs. 

\subsection{Non standardized extensions}

Interestingly, the posix standard does not define other scheduling types than other, fifo and RR. 
There is no mentioning of deadline, iso, or batch. 
At the same time, the standard foresees a structure defining the scheduler's parameters, but it contains only one parameter, the absolute scheduling priority.

This might be taken into consideration when trying to run the orchestrator in a non Linux environment.

\section{Manipulating processes}

In this part we are mainly interested in manipulating process priorities and scheduling behavior. 
The POSIX calls and standards are investigated to verify possibilities.

\subsection{PID namespaces}

Before considering option of sharing PID space with host, see if we can access from a privileged container nonetheless the namespace isolation. 
Posix defines functions ioctrl\_ns to determine the parent namespace of the actual one. can help getting the inode of a parent namespace and read information. 
if pids can also be read is a matter of investigation. maybe mount as \/procext?

what information can be read out from the pid is still to be investigated.

Manuals say that a parent can read child namespaces but not vice-versa. All enquiries of pared pid in fact once reached pid 1 (must not be 1 for a container) end up with a 0 response.

\subsection{PID information}
\label{sub:pidinf}

While in UNIX the PID table and information is part of the kernel memory, in linux the only way to access the process information is by parsing virtual files. 
If the orchestrator has to run on both systems, the PID retrieval must thus be implemented twice. Once via /proc pipe readout and once via sysctrl calls to read kernel memory.

It is highly likely that the same is true for the other resources, such as memory or connectivity. 

An example code has been tested which gives the PID's of all matching process names. 
It might therefore be possible to poll once and verify cyclically for unbound containers. 

A hook-like structure might be more convenient. Such modification would require a kernel change and is therefore considered only in a second moment.

\subsection{Scheduling of processes}

As default there is only limited access to the scheduling activities of a process.
In the POSIX documentation we find the following functions:

\begin{itemize}
	\item \textbf{sched\_get\_priority\_max(int)}
		gets the max priority of the specified scheduling policy. On Linux, it is 0 for SCHED\_OTHER and 1 for everything else. 
	
	\item \textbf{sched\_get\_priority\_min(int)}
		gets the min priority of the specified scheduling policy. On Linux, it is 0 for SCHED\_OTHER and 99 for everything else. 
		
	\item \textbf{sched\_getparam(pid\_t, struct sched\_param *)}
		gets the scheduling parameters of a specified process ID
	
	\item \textbf{sched\_getscheduler(pid\_t)}
		gets the set scheduling policy for a PID
	
	\item \textbf{sched\_rr\_get\_interval(pid\_t, struct timespec *)}
		gets the length of the quantum used when using the round robin scheduling approach for realtime.
		With a Linux kernel, the round robin time slice is always 150 microseconds, and pid need not even be a real pid. At least that is what the manuals say. Interrestingly a test has shown that there are also 8000 or 16000 $\mu s$ sometimes returned when querying the value.
	
	\item \textbf{sched\_setparam(pid\_t, const struct sched\_param *)}
		sets the parameters for a specified process ID
	
	\item \textbf{sched\_setscheduler(pid\_t, int, const struct sched\_param *)}
		sets the scheduling policy for a PID
	
	\item \textbf{sched\_yield(void)}
		return the control to the scheduler (leave the execution state).
\end{itemize}

In addition, if the scheduling is set to traditional, i.e. SCHED\_OTHER, the niceness value for the complete fair scheduler can be adjusted with additional functions. 

Posix as said, implements only three scheduling policies.
Linux in addition has now:

\begin{verbatim}
#define SCHED\_BATCH		3
/* SCHED\_ISO: reserved but not implemented yet */
#define SCHED\_IDLE		5
#define SCHED\_DEADLINE		6
\end{verbatim}


In addition, scheduling attributes are now extended as:

\begin{verbatim}
struct sched\_attr {
	__u32 size;
	
	__u32 sched\_policy;
	__u64 sched\_flags;
	
	/* SCHED\_NORMAL, SCHED\_BATCH */
	__s32 sched\_nice;
	
	/* SCHED\_FIFO, SCHED\_RR */
	__u32 sched_priority;
	
	/* SCHED\_DEADLINE */
	__u64 sched\_runtime;
	__u64 sched\_deadline;
	__u64 sched\_period;
};
\end{verbatim}

and might be read and written as follows:


\begin{verbatim}
int sched_setattr(pid_t pid, struct sched_attr *attr,
	unsigned int flags);

int sched_getattr(pid_t pid, struct sched_attr *attr,
	unsigned int size, unsigned int flags);
\end{verbatim}

This structure is in addition to the original sched\_param which has been kept for standardization and backwards compatibility issues.

The commented scheduling mode in the list, i.e. SCHED\_ISO, is a scheduler intended for isochronous (burst-like) tasks and has not been implemented yet. 
The brainfuck scheduler (BFS) developed by Con Kolivas is anltered version witch actually supports the ISO scheduling mode, with priority between RT and Normal/Other tasks.
For further details see "A complete guide to Linux process scheduling"  by Nikita Ishkov.


\subsection{Other considerations}

During the exploration of the namespace and pid function, some features to be implemented in future where discovered. 
The PID 1 inside the container is parent of all the namespace. If it dies, all the children are orphaned and will be killed. As parent, it is also target of sigs without implicit binding, thus it must manually handle incoming signals such as sig\_int.
The userspace in the container can be mapped. To avoid root as well as other accesses from a container, a proper map should be set up and incorporated in all the containers at load.

\section{Test implementation}

The orchestrator will have to interact with the scheduler while monitoring the tasks. 
Ideally, the interaction is mutual. Unfortunately, no hint has been found suggesting that there is a way to ``register'' to the scheduler to get scheduling and process updates. 
Thus, apart from the system calls for getting and setting a process's scheduling attributes, there is no direct communication with the kernel. 
This means that the PIDs of all the processes must be queried manually, cyclically, considering time trends and thus the orchestrator must keep PID history. 

\subsection{PID search}

In a first implementation test, we verify therefore the poll quality and speed of pid queries.
As described in Section \ref{sub:pidinf}, there are two ways of polling the prcess table. 
In our testing environment we use Linux, and are therefore running for a virtual filesystems parsing. 
To ease the parsing, we can use the integrated tool ``pidof'' via a pipe opened throught the call of \texttt{popen}.
\texttt{popen} is a POSIX standard call that opens a one-way pipe to indicated  purposely process called via shell.

As soon as the orchestrator is up and running, it scans for target processes which might be  defined in a configuration file (\textit{cmdfile}).
The PID list is parsed and verified. 
The whole process takes a fraction of a ms and does not put load on the cpu. With unlimited loop the {CPU}-load is below 3\%.

An option to optimize process verification might be that the environment controlling the containers informs the orchestrator about containers entering or leaving the scheduler.
This way, an orchestrator does not have to scan for new PID all the time.
Kubernetes, for example, foresees the use of postStart and preStop handlers when installing a pod. 
The handlers can be specified in the yaml configuration file of the POD for each container. A Typical use for this might be the shutting down or preparation for a service such as a web server. 
In our case we could use it to signal a process start or stop.
More details about this configuration can be \href{{https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/}}{found here}.
The Docker API on the other hand does not implement a pre- or post execution handler for container start or stop. 
Only the container configuration itself could contain a run command as part of the container to be executed upon startup. This might be also a standard configuration setting, signaling the orchestrator the presence of a new container process.

 



\end{document}
