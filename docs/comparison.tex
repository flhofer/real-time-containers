\documentclass[]{scrartcl}

\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{multirow}

%opening
\title{Comparison of Kubernetes with the proposed orchestration approach}
\author{Florian Hofer}

\begin{document}

\maketitle

\section{Kubernetes}

Kubernetes is a well-known container management platform developed by Google. It is able manage multiple containers also distributed on various hosts. The advantage is for sure the offered centralized control portal. 

In Kubernetes the resource management is performed previous to container deployment. Similar to docker-compose, the resources such as memory and CPU-limit can be specified in a JSON based configuration file.
The configuration file describes the resource requests and upper limits for a set of containers, also called a Pod. 
For these settings there are three combinations, fitting into three Quality of Service classes: (see ~\href{https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md}{Kubernetes QoS})
% ADd information about QoS management of extras, and overshoots, margin can be set by observaton.
\begin{itemize}
	\item Best-effort: limits and requests are not set, thus all the pods and containers are run in a best effort manner.
	These are also the lowest priority and the first pods to be killed when a system runs low of memory.
	\item Burst: for some containers, requests and optionally limits are set greater than 0 and not equal for one or more resources.
	Here a minimum of resources are guaranteed, which can top up until they reach the configuration or system limit. Once threre are no more best-effort pods available, these are the most likely pods to be killed in system memory distress.
	\item Guaranteed: limits and optionally requests are set greater than 0 and equal for all resources and containers, the pod is said to be guaranteed. 
	These pods run top priority and are not to be killed until either they exceed their memory limits or there are no lower priority pods to be depleted. 
\end{itemize}
In all cases, if CPU-time can not be met, the process will be throttled down and not killed. 
The memory distress behavior of the classes can be defined by a score system. 
The Out Of Memory score is again determined by the QoS class: a best effort pod has by default a score of 1000, while a guaranteed pod has a worst case score of 1. The limits of OOM score are 0 to 1000, thus in a OOM scenario, the best effort containers are the first to be killed by the linux OOM killer.

The required resources of a Pod are actually the request amount for the run of the whole container collection, thus the sum of resources. All resource requests of the pod must be fulfilled for the containers to be deployed.

The information about Kubernetes has been taken from the
\href{https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/}{Kubernetes Documentation}.


\section{Orchestration engine}

The orchestration engine, different from Kubernetes, is a management tool that works directly on the scheduling level. The planned application code will run in a container in the same engine as the other real-time applications. 
As such, the orchestrator runs in an ephemeral unit which is easily replaceable and upgradeable. An eventual additional tuning configuration can be deployed on the host and mounted as volume inside the container. 
A downside of the solution is that the engine can operate just on the local host.

In the previous example with Kubernetes, the configuration of the resources has been realized through requests and limits in a JSON file. 
The orchestration engine, on the other hand, will work with task data and their real-time properties. 
Thus, the input parameters are start-time, runtime, period and eventually deadline in addition to the regular task priority.
All the tasks will be managed separately, giving the tool little more flexibility in local deployment of CPU-time.
In addition, the continuous monitoring and interaction with the system scheduler will give the orchestrator the flexibility to reschedule containers during runtime.

\section{Comparison}

Table \ref{tab:approach} gives a quick overview of the technologies in comparison.
The advantage of Kubernetes is the centralized management of containers. 
The limits for each container can be set in a group configuration and then downloaded to the hosts. 
The disadvantage here is that the limits must be set big enough to manage eventual overshoots. 
Thus, the CPU-time must be set to the maximum expected and/or measures or an overload caused by a overshoot will never be recovered again.
Kubernetes can not manage such situations as it is not directly acting on the scheduler but only on the offline configuration. 
An overshooting thread will be throttled down, an undesirable state in any realtime system.
Thus, in the event of doubt, the CPU-time limits should be set on the higher end of a monitored maximum runtime.

The orchestration engine on the other hand, knows the tasks to be run, their properties and deadlines.
In the dynamic version it will also be able to shift tasks to other computation units and relocate resources when needed.
The advantage here is that a critical situation sourcing from a container's task overload might be avoided.
The complexity of this approach is for sure higher and while Kubernetes might simply set control group limits on the container tasks, the orchestrator has to continuously monitor the exectuon of the tasks.
On the other hand, this monitoring brings the advantage that the engine always knows status and limits of the system. 
Hence, it is possible to manage resources in a drastically more efficient manner. The computation resource bounds can be set more agressively with %task deadlines as upper bound and 
task runtimes as lower bound. 
The resulting resource allocation might thus be manifold.

\begin{table}[ht]
	\centering
	\begin{tabular}{l c c c}
		 \multirow{2}{*}{Property} & \multirow{2}{*}{Kubernetes} & \multicolumn{2}{c}{Orchestrator} \\
		& & static & dynamic \\
		\toprule
		Action range & Cluster & Local* & Local* \\
		Configuration & JSON & Task + file (opt) & Task + file (opt) \\
		Interface & CLI + WebUI & CLI & CLI \\
		\midrule
		Controls & CPU, memory & CPU, memory & CPU, memory\\
		Ctrl. type & static & static & dynamic\\
		Groups & Pods & cgroups & cgroups \\
		Realtime aware & no & yes & yes\\
		\midrule
		Upper limits & expected max & Config & Deadline \\
		Overshoots & Critical & Critical/Contained & Managed \\
		\midrule
		Tasks per CPU & 3 & 3-50* & 50-100*\\
		Configured limit & 300ml & 300ml & 10-20ml\\
		Configuted request & 10 ml & 10 ml & 10 ml \\
		Resource buffer CPU-time & 970ml & 970ml & 500-0ml*\\
		\bottomrule
	\end{tabular}
	\caption{Comparison table of the different management approaches. A sample task of 10ml runtime,
		20ml deadline and 300\% overshoots is considered for the last group}
	\label{tab:approach}
\end{table}

But the advantages are not finished here: 
The table clearly shows that the two approaches are different and not concurrent. If CPU limitation and affinity is not applied, the two solutions together can exploit advantages on both sides. While it might be true that also the orchestrator could, with the help of Cgrpups. add also memory management including numa node management, the cooperation of the remote management tool and the integration of the orchestratior with the kubelet agent might tbe easiest and most profitable solution. 

\section{Conclusions}

The two solutions are different in kind. While a static approach would get very similar results, the dynamic orchestrator could reach higher resource economy by allocating a higher number of tasks on the same resources.
The feasibility of a combined approach and the actual performance of the orchestrator are still to be determined, anyway, the data seems promising.


\end{document}
